{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd230b11",
   "metadata": {},
   "source": [
    "HOPFIELD NETWORK: FROM HANDWRITTEN DIGITS TO DAYDREAMING\n",
    "\n",
    "Goal: demonstration of associative memory and capacity enhancement of daydreaming algorithm\n",
    "\n",
    "Cell Structure:\n",
    "1. Functions for loading data, implementing hopfield NN, and implementing daydreaming \n",
    "2. Demonstrate HNN pattern memorization on handwritten digits\n",
    "3. Explore how correlation affects memory capcity \n",
    "3. Demonstrate HNN on uncorrelated data Salt & Pepper Patterns - compare reconstruction methods\n",
    "4. Compare daydreaming algorithm memory capacity with Hebbian learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f1156d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mnist\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m zoom\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#create/load in inital conditions\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "###----Functions for implementing hopfield NN and daydreaming algorithm \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "#create/load in inital conditions\n",
    "def load_img(path, side):\n",
    "    \"\"\"Load image from disk and convert to binary {-1, +1} vector.\"\"\"\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((side, side))\n",
    "    img = img.convert('1')  # thresholded black/white\n",
    "    arr = 2 * np.array(img, dtype=np.int8) - 1\n",
    "    return arr.flatten()\n",
    "\n",
    "def show_array(img_array, title=None):\n",
    "    \"\"\"Display a single pattern (1D vector) as an image.\"\"\"\n",
    "    side = int(np.sqrt(img_array.shape[0]))\n",
    "    arr = img_array.reshape((side, side))\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(arr, cmap='gray', vmin=-1, vmax=1)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def show_multiple_arrays(img_arrays, titles=None, suptitle=None):\n",
    "    \"\"\"Display multiple patterns side by side.\"\"\"\n",
    "    k = len(img_arrays)\n",
    "    fig, axes = plt.subplots(1, k, figsize=(3*k, 3))\n",
    "    if k == 1:\n",
    "        axes = [axes]\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle, fontsize=14, y=1.02)\n",
    "\n",
    "    for i in range(k):\n",
    "        side = int(np.sqrt(img_arrays[i].shape[0]))\n",
    "        axes[i].imshow(img_arrays[i].reshape((side, side)), cmap='gray', vmin=-1, vmax=1)\n",
    "        if titles and i < len(titles):\n",
    "            axes[i].set_title(titles[i])\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def modify_img(n, img, flip_frac=0.3, rng=None):\n",
    "    \"\"\"Add noise by flipping a fraction of pixels.\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    img_noisy = img.copy()\n",
    "    m = int(flip_frac * n)\n",
    "    idx = rng.integers(0, n, size=m, endpoint=False)\n",
    "    img_noisy[idx] *= -1\n",
    "    return img_noisy\n",
    "\n",
    "def pixel_accuracy(reconstructed, target):\n",
    "    \"\"\"Calculate percentage of correct pixels.\"\"\"\n",
    "    return 100.0 * np.sum(reconstructed == target) / target.size\n",
    "\n",
    "# Hopfield \n",
    "def calculate_w(img):\n",
    "    \"\"\"Hebbian outer product for a single pattern.\"\"\"\n",
    "    return np.outer(img, img)\n",
    "\n",
    "def build_weight_matrix(images, scale_by_n=True, dtype=np.float32):\n",
    "    \"\"\"\n",
    "    Build weight matrix using Hebbian learning over a list of patterns.\n",
    "    images: list of 1D arrays of length n with entries in {-1, +1}.\n",
    "    \"\"\"\n",
    "    n = images[0].size\n",
    "    w = np.zeros((n, n), dtype=dtype)\n",
    "    for im in images:\n",
    "        w += calculate_w(im).astype(dtype)\n",
    "    if scale_by_n:\n",
    "        w /= n\n",
    "    np.fill_diagonal(w, 0.0)  # No self-coupling\n",
    "    w = 0.5 * (w + w.T)       # Enforce symmetry\n",
    "    return w\n",
    "\n",
    "def energy(w, state, b=None):\n",
    "    \"\"\"Calculate Hopfield energy E = -1/2 s^T W s - b^T s (if bias present).\"\"\"\n",
    "    e = -0.5 * state @ (w @ state)\n",
    "    if b is not None:\n",
    "        if np.ndim(b) == 0:\n",
    "            e -= b * np.sum(state)\n",
    "        else:\n",
    "            e -= b @ state\n",
    "    return float(e)\n",
    "\n",
    "def overlap(state, pattern):\n",
    "    \"\"\"Calculate overlap (normalized dot product) with a stored pattern.\"\"\"\n",
    "    return float((state @ pattern) / state.size)\n",
    "\n",
    "\n",
    "def reconstructed_image_async(n, w, state, ts=50_000, patience=1_000):\n",
    "    \"\"\"\n",
    "    Asynchronous deterministic update (T=0).\n",
    "    Picks a random neuron, updates it according to sign(h_i).\n",
    "    Stops when we see 'patience' consecutive no-change steps or after ts steps.\n",
    "    \"\"\"\n",
    "    unchanged = 0\n",
    "    for _ in range(ts):\n",
    "        i = np.random.randint(0, n)\n",
    "        h = np.dot(w[i, :], state)\n",
    "        s_new = 1 if h > 0 else -1\n",
    "        if s_new != state[i]:\n",
    "            state[i] = s_new\n",
    "            unchanged = 0\n",
    "        else:\n",
    "            unchanged += 1\n",
    "        if unchanged > patience:\n",
    "            break\n",
    "    return state\n",
    "\n",
    "def metropolis_step_fast(w, state, beta):\n",
    "    \"\"\"One Metropolis update step at inverse temperature beta.\"\"\"\n",
    "    n = state.size\n",
    "    i = np.random.randint(0, n)\n",
    "    h_i = np.dot(w[i, :], state)\n",
    "    dE = 2.0 * state[i] * h_i\n",
    "    if dE <= 0.0 or np.random.rand() < np.exp(-beta * dE):\n",
    "        state[i] = -state[i]\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def reconstructed_image_metropolis(n, w, state, T=0.6, ts=50_000, patience=2_000):\n",
    "    \"\"\"\n",
    "    Asynchronous Metropolis at fixed temperature T.\n",
    "    Useful for escaping shallow local minima.\n",
    "    \"\"\"\n",
    "    beta = 1.0 / max(1e-12, T)\n",
    "    unchanged = 0\n",
    "    for _ in range(ts):\n",
    "        flipped = metropolis_step_fast(w, state, beta)\n",
    "        if flipped:\n",
    "            unchanged = 0\n",
    "        else:\n",
    "            unchanged += 1\n",
    "        if unchanged > patience:\n",
    "            break\n",
    "    return state\n",
    "\n",
    "def relax_zero_temperature(w, state, ts=50_000, patience=1_000, rng=None):\n",
    "    \"\"\"Asynchronous T=0 dynamics to fixed point.\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    n = state.size\n",
    "    unchanged = 0\n",
    "\n",
    "    for _ in range(ts):\n",
    "        i = rng.integers(0, n)\n",
    "        h = np.dot(w[i, :], state)\n",
    "        s_new = 1 if h > 0 else -1\n",
    "        if s_new != state[i]:\n",
    "            state[i] = s_new\n",
    "            unchanged = 0\n",
    "        else:\n",
    "            unchanged += 1\n",
    "        if unchanged > patience:\n",
    "            break\n",
    "    return state\n",
    "\n",
    "# Daydreaming\n",
    "def daydream_step(w, patterns, tau=50.0, relax_steps=50_000,\n",
    "                  patience=1_000, rng=None):\n",
    "    \"\"\"\n",
    "    One Daydreaming update step, roughly following the idea:\n",
    "      1. Pick a stored pattern xi.\n",
    "      2. Start from random state, relax to a fixed point under current W.\n",
    "      3. Compare outer products xi xi^T and s s^T and update W toward xi and away from s.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    n = w.shape[0]\n",
    "    mu = rng.integers(0, len(patterns))  # choose pattern index\n",
    "    xi = patterns[mu]\n",
    "\n",
    "    # Start from random state and relax under current dynamics\n",
    "    s = rng.choice([-1, 1], size=n)\n",
    "    s = relax_zero_temperature(w, s, ts=relax_steps, patience=patience, rng=rng)\n",
    "\n",
    "    # Daydream update rule\n",
    "    delta_w = (np.outer(xi, xi) - np.outer(s, s)) / (tau * n)\n",
    "    w += delta_w\n",
    "\n",
    "    # Re-enforce constraints on W\n",
    "    np.fill_diagonal(w, 0.0)\n",
    "    w[:] = 0.5 * (w + w.T)\n",
    "\n",
    "    return w\n",
    "\n",
    "def normalize_w(w):\n",
    "    \"\"\"Normalize weight matrix by RMS to avoid unbounded growth.\"\"\"\n",
    "    rms = np.sqrt(np.mean(w**2))\n",
    "    if rms > 0:\n",
    "        w /= rms\n",
    "    return w\n",
    "\n",
    "def train_daydreaming(patterns, tau=50.0, n_epochs=20, steps_per_epoch=100,\n",
    "                      relax_steps=50_000, patience=1_000, scale_by_n=True,\n",
    "                      dtype=np.float32, rng=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Train weight matrix using Daydreaming algorithm starting from Hebbian W.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    n = patterns[0].size\n",
    "    w = build_weight_matrix(patterns, scale_by_n=scale_by_n, dtype=dtype)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for _ in range(steps_per_epoch):\n",
    "            w = daydream_step(w, patterns, tau=tau, relax_steps=relax_steps,\n",
    "                              patience=patience, rng=rng)\n",
    "        w = normalize_w(w)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  Epoch {epoch+1}/{n_epochs} complete\")\n",
    "\n",
    "    return w\n",
    "\n",
    "# data analysis functions\n",
    "def generate_random_patterns(n, P, rng=None):\n",
    "    \"\"\"Generate P random binary patterns for capacity experiments.\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    patterns = rng.choice([-1, 1], size=(P, n))\n",
    "    return [patterns[i] for i in range(P)]\n",
    "\n",
    "def retrieve_success_rate(w, patterns, flip_frac=0.3, n_trials_per_pattern=5,\n",
    "                          overlap_threshold=0.9, ts=50_000, patience=1_000, rng=None):\n",
    "    \"\"\"\n",
    "    Measure retrieval success rate for a given weight matrix and a set of patterns.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    n = patterns[0].size\n",
    "    successes = 0\n",
    "    total = 0\n",
    "\n",
    "    for p in patterns:\n",
    "        for _ in range(n_trials_per_pattern):\n",
    "            s0 = modify_img(n, p, flip_frac=flip_frac, rng=rng)\n",
    "            s_final = relax_zero_temperature(w, s0.copy(), ts=ts,\n",
    "                                             patience=patience, rng=rng)\n",
    "            m = overlap(s_final, p)\n",
    "            if m >= overlap_threshold:\n",
    "                successes += 1\n",
    "            total += 1\n",
    "\n",
    "    return successes / total if total > 0 else 0.0\n",
    "\n",
    "#image processing\n",
    "def upscale_digit(img_28x28, target_size):\n",
    "    \"\"\"Upscale 28x28 digit to larger size using nearest-neighbor.\"\"\"\n",
    "    scale = target_size / img_28x28.shape[0]\n",
    "    upscaled = zoom(img_28x28, scale, order=0)  # order=0 = nearest neighbor\n",
    "    return upscaled\n",
    "\n",
    "#generate salt and pepper\n",
    "def generate_salt_pepper_pattern(side=50, seed=None):\n",
    "    \"\"\"Generate random binary pattern {-1, +1} on a square grid.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pattern = rng.integers(0, 2, size=(side, side), dtype=np.int8)\n",
    "    pattern = 2 * pattern - 1\n",
    "    return pattern\n",
    "\n",
    "def save_salt_pepper_pattern(pattern, filename):\n",
    "    \"\"\"Save pattern as image file (0/255 grayscale).\"\"\"\n",
    "    img_array = ((pattern.astype(np.int32) + 1) // 2 * 255).astype(np.uint8)\n",
    "    img = Image.fromarray(img_array, mode='L')\n",
    "    img.save(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a5f95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###----Test normal HNN on handwritten images from Mniest\n",
    "\n",
    "# Load MNIST dataset (28x28 grayscale digit images)\n",
    "(x_train, y_train), _ = mnist.load_data()\n",
    "\n",
    "original_size = 28  # MNIST digits are 28x28\n",
    "\n",
    "# ADJUSTABLE: desired digit size for the Hopfield network\n",
    "digit_size = 100  # can also try 50, 80, 128, etc.\n",
    "\n",
    "# Select one example of each of a few digits (e.g., 0,1,2,3,4)\n",
    "digits_to_store = [0, 1, 2, 4, 8]\n",
    "digit_patterns = []\n",
    "stored_labels = []\n",
    "\n",
    "print(f\"Upscaling MNIST digits from {original_size}×{original_size} to {digit_size}×{digit_size}\")\n",
    "print(\"Storing these handwritten digits:\")\n",
    "\n",
    "fig, axes = plt.subplots(1, len(digits_to_store), figsize=(3*len(digits_to_store), 3))\n",
    "\n",
    "for idx, d in enumerate(digits_to_store):\n",
    "    # Pick the first training example with label d\n",
    "    img_28 = x_train[y_train == d][0]     # shape (28, 28)\n",
    "\n",
    "    # Upscale to desired resolution\n",
    "    img = upscale_digit(img_28, digit_size)  # shape (digit_size, digit_size)\n",
    "\n",
    "    # Binarize: threshold at mean\n",
    "    binary = np.where(img > img.mean(), 1, -1).flatten()\n",
    "\n",
    "    digit_patterns.append(binary)\n",
    "    stored_labels.append(d)\n",
    "\n",
    "    axes[idx].imshow(img, cmap='gray')\n",
    "    axes[idx].set_title(f'Digit: {d}')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(f\"Stored Patterns (MNIST digits {digit_size}×{digit_size})\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Build Hopfield network for these patterns\n",
    "print(f\"\\nBuilding Hopfield network with {len(digit_patterns)} stored patterns...\")\n",
    "n_digits = digit_size * digit_size\n",
    "print(f\"  Each pattern is {digit_size}×{digit_size} = {n_digits} pixels\")\n",
    "print(\"  This implies a weight matrix of size \"\n",
    "      f\"{n_digits}×{n_digits} (~{n_digits*n_digits*4/1e6:.1f} MB in float32)\\n\")\n",
    "\n",
    "w_digits = build_weight_matrix(digit_patterns, scale_by_n=True, dtype=np.float32)\n",
    "print(f\"Network size: {n_digits} neurons\")\n",
    "print(f\"Weight matrix shape: {w_digits.shape}\\n\")\n",
    "\n",
    "# Test retrieval with noisy input on one of the stored digits\n",
    "test_digit_idx = 2  # index in digits_to_store list (here digit \"2\")\n",
    "test_pattern = digit_patterns[test_digit_idx]\n",
    "label = stored_labels[test_digit_idx]\n",
    "noise_level = 0.30\n",
    "\n",
    "print(f\"Testing retrieval on digit '{label}' with {noise_level*100:.0f}% noise added...\\n\")\n",
    "noisy_test = modify_img(n_digits, test_pattern, flip_frac=noise_level)\n",
    "\n",
    "# Reconstruct using asynchronous update\n",
    "reconstructed = reconstructed_image_async(\n",
    "    n_digits, w_digits, noisy_test.copy(),\n",
    "    ts=20_000, patience=1_000\n",
    ")\n",
    "\n",
    "# Visualize original, noisy, and reconstructed patterns\n",
    "accuracy = pixel_accuracy(reconstructed, test_pattern)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(test_pattern.reshape(digit_size, digit_size), cmap='gray', vmin=-1, vmax=1)\n",
    "axes[0].set_title(f'Original\\nDigit: {label}')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(noisy_test.reshape(digit_size, digit_size), cmap='gray', vmin=-1, vmax=1)\n",
    "axes[1].set_title(f'Noisy Input\\n({noise_level*100:.0f}% corrupted)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(reconstructed.reshape(digit_size, digit_size), cmap='gray', vmin=-1, vmax=1)\n",
    "axes[2].set_title(f'Reconstructed\\n({accuracy:.1f}% accurate)')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle(\"Hopfield Network: High-Resolution Pattern Retrieval (MNIST 100×100)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"  Energy (initial noisy state): {energy(w_digits, noisy_test):.4f}\")\n",
    "print(f\"  Energy (final state):         {energy(w_digits, reconstructed):.4f}\")\n",
    "print(f\"  Overlap with original:        {overlap(reconstructed, test_pattern):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2ce27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###----Determine Correlation between patterns\n",
    "## How does correlation affect memory?\n",
    "\n",
    "# Calculate correlation between stored digits\n",
    "print(\"Analyzing pattern correlations:\")\n",
    "correlations = []\n",
    "for i in range(len(digit_patterns)):\n",
    "    for j in range(i+1, len(digit_patterns)):\n",
    "        corr = overlap(digit_patterns[i], digit_patterns[j])\n",
    "        correlations.append(abs(corr))\n",
    "        print(f\"  |Overlap(Pattern {i}, Pattern {j})| = {abs(corr):.3f}\")\n",
    "\n",
    "avg_corr = np.mean(correlations)\n",
    "print(f\"\\nAverage correlation: {avg_corr:.3f}\")\n",
    "print(f\"High correlation → Limited capacity!\\n\")\n",
    "\n",
    "# Test with increasing number of digits\n",
    "print(\"Testing capacity with increasing number of stored digits:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'# Patterns':<12} {'Success Rate':<15} {'Note'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for n_store in [3, 5, 7, 10, 15]:\n",
    "    if n_store > len(digits.target):\n",
    "        break\n",
    "\n",
    "    # Select n_store digits\n",
    "    test_patterns = []\n",
    "    for i in range(n_store):\n",
    "        img = digits.images[i * 10]  # Take every 10th to get variety\n",
    "        binary = np.where(img > img.mean(), 1, -1).flatten()\n",
    "        test_patterns.append(binary)\n",
    "\n",
    "    # Build network and test\n",
    "    w_test = build_weight_matrix(test_patterns, scale_by_n=True)\n",
    "    success_rate = retrieve_success_rate(w_test, test_patterns, flip_frac=0.25,\n",
    "                                         n_trials_per_pattern=3, overlap_threshold=0.9)\n",
    "\n",
    "    note = \"Good\" if success_rate > 0.8 else \"Degraded\" if success_rate > 0.5 else \"Failed\"\n",
    "    print(f\"{n_store:<12} {success_rate:<15.2%} {note}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92c48fb",
   "metadata": {},
   "source": [
    "We see that high correlation between the images reduces the memory capcity, so we want to consider lower correlation images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a2c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "###---Use uncorrelated patterns (salt and pepper noise) to try to improve capacity\n",
    "\n",
    "\n",
    "# Generate salt & pepper patterns\n",
    "print(\"Generating random salt & pepper patterns...\")\n",
    "side = 50\n",
    "pattern_seeds = [42, 123, 568, 34]\n",
    "sp_patterns = [generate_salt_pepper_pattern(side, seed=s) for s in pattern_seeds]\n",
    "\n",
    "# Save patterns\n",
    "for i, pattern in enumerate(sp_patterns, 1):\n",
    "    save_salt_pepper_pattern(pattern, f'p{i}.jpg')\n",
    "print(f\"✓ Generated {len(sp_patterns)} patterns ({side}×{side} pixels)\\n\")\n",
    "\n",
    "# Display stored patterns\n",
    "print(\"Stored patterns:\")\n",
    "show_multiple_arrays([p.flatten() for p in sp_patterns],\n",
    "                    titles=[f'Pattern {i+1}' for i in range(len(sp_patterns))],\n",
    "                    suptitle=\"Salt & Pepper Patterns (Uncorrelated)\")\n",
    "\n",
    "# Build Hopfield network\n",
    "n_sp = side * side\n",
    "imgs_sp = [p.flatten() for p in sp_patterns]\n",
    "w_sp = build_weight_matrix(imgs_sp, scale_by_n=True, dtype=np.float32)\n",
    "\n",
    "print(f\"\\nNetwork configuration:\")\n",
    "print(f\"  Size: {n_sp} neurons ({side}×{side})\")\n",
    "print(f\"  Stored patterns: {len(imgs_sp)}\")\n",
    "print(f\"  Weight matrix: {w_sp.shape}\\n\")\n",
    "\n",
    "# Test reconstruction with different methods\n",
    "target_idx = 2\n",
    "target = imgs_sp[target_idx]\n",
    "flip_frac = 0.40\n",
    "fixed_T = 0.6\n",
    "\n",
    "print(f\"Testing reconstruction methods:\")\n",
    "print(f\"  Target: Pattern {target_idx+1}\")\n",
    "print(f\"  Noise level: {flip_frac*100:.0f}% pixels flipped\\n\")\n",
    "\n",
    "# Create noisy version\n",
    "state0 = modify_img(n_sp, target, flip_frac=flip_frac)\n",
    "init_acc = pixel_accuracy(state0, target)\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Method':<30} {'Accuracy':<12} {'Energy':<12} {'Overlap'}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Initial (noisy)':<30} {init_acc:>6.2f}%     {energy(w_sp, state0):>10.4f}  {overlap(state0, target):>7.4f}\")\n",
    "\n",
    "# Method 1: Asynchronous (T=0)\n",
    "s_async = reconstructed_image_async(n_sp, w_sp, state0.copy(), ts=50_000, patience=2_000)\n",
    "async_acc = pixel_accuracy(s_async, target)\n",
    "print(f\"{'Asynchronous (T=0)':<30} {async_acc:>6.2f}%     {energy(w_sp, s_async):>10.4f}  {overlap(s_async, target):>7.4f}\")\n",
    "\n",
    "# Method 2: Metropolis\n",
    "s_met = reconstructed_image_metropolis(n_sp, w_sp, state0.copy(), T=fixed_T,\n",
    "                                       ts=80_000, patience=3_000)\n",
    "met_acc = pixel_accuracy(s_met, target)\n",
    "print(f\"{'Metropolis (T='+str(fixed_T)+')':<30} {met_acc:>6.2f}%     {energy(w_sp, s_met):>10.4f}  {overlap(s_met, target):>7.4f}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "axes[0].imshow(target.reshape(side, side), cmap='gray', vmin=-1, vmax=1)\n",
    "axes[0].set_title(f'Original Target')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(state0.reshape(side, side), cmap='gray', vmin=-1, vmax=1)\n",
    "axes[1].set_title(f'Noisy Input\\n{init_acc:.1f}% accurate')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(s_async.reshape(side, side), cmap='gray', vmin=-1, vmax=1)\n",
    "axes[2].set_title(f'Asynchronous\\n{async_acc:.1f}% accurate')\n",
    "axes[2].axis('off')\n",
    "\n",
    "axes[3].imshow(s_met.reshape(side, side), cmap='gray', vmin=-1, vmax=1)\n",
    "axes[3].set_title(f'Metropolis (T={fixed_T})\\n{met_acc:.1f}% accurate')\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.suptitle(\"Reconstruction Method Comparison\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_method = \"Asynchronous\" if async_acc > met_acc else \"Metropolis\"\n",
    "print(f\"Best method: {best_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd1b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "###---Compare memory capacity for daydreaming with normal HNN\n",
    "\n",
    "# Configuration\n",
    "side_cap = 20\n",
    "n_cap = side_cap * side_cap\n",
    "alpha_list = [0.02, 0.05, 0.08, 0.11, 0.14, 0.17, 0.20]\n",
    "\n",
    "print(\"Memory capacity experiment:\")\n",
    "print(f\"  Network size: {n_cap} neurons ({side_cap}×{side_cap})\")\n",
    "print(f\"  Noise level: 30% pixels flipped\")\n",
    "print(f\"  Success threshold: 90% overlap\\n\")\n",
    "\n",
    "print(\"Running capacity comparison (this may take a few minutes)...\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Load α':<10} {'# Patterns':<12} {'Hebbian':<15} {'Daydreaming':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "hebb_results = []\n",
    "dd_results = []\n",
    "\n",
    "for alpha in alpha_list:\n",
    "    P = max(1, int(round(alpha * n_cap)))\n",
    "    patterns = generate_random_patterns(n_cap, P, rng=rng)\n",
    "\n",
    "    # Hebbian\n",
    "    w_hebb = build_weight_matrix(patterns, scale_by_n=True, dtype=np.float32)\n",
    "    hebb_sr = retrieve_success_rate(w_hebb, patterns, flip_frac=0.3,\n",
    "                                   n_trials_per_pattern=5, overlap_threshold=0.9,\n",
    "                                   ts=20_000, patience=1_000, rng=rng)\n",
    "    hebb_results.append(hebb_sr)\n",
    "\n",
    "    # Daydreaming\n",
    "    w_dd = train_daydreaming(patterns, tau=50.0, n_epochs=10, steps_per_epoch=50,\n",
    "                            relax_steps=5_000, patience=500, scale_by_n=True,\n",
    "                            dtype=np.float32, verbose=False, rng=rng)\n",
    "    dd_sr = retrieve_success_rate(w_dd, patterns, flip_frac=0.3,\n",
    "                                 n_trials_per_pattern=5, overlap_threshold=0.9,\n",
    "                                 ts=20_000, patience=1_000, rng=rng)\n",
    "    dd_results.append(dd_sr)\n",
    "\n",
    "    print(f\"{alpha:<10.3f} {P:<12} {hebb_sr:<15.2%} {dd_sr:<15.2%}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(alpha_list, hebb_results, 'o-', linewidth=2, markersize=8,\n",
    "         label='Hebbian Learning', color='#2E86AB')\n",
    "plt.plot(alpha_list, dd_results, 's-', linewidth=2, markersize=8,\n",
    "         label='Daydreaming Algorithm', color='#A23B72')\n",
    "plt.axhline(y=0.9, color='gray', linestyle='--', alpha=0.5, label='90% Success Threshold')\n",
    "plt.xlabel('Load α = P/N (Patterns per Neuron)', fontsize=12)\n",
    "plt.ylabel('Retrieval Success Rate', fontsize=12)\n",
    "plt.title('Memory Capacity: Hebbian vs Daydreaming', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS & CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find capacity thresholds\n",
    "hebb_capacity = next((alpha for alpha, sr in zip(alpha_list, hebb_results) if sr < 0.9), alpha_list[-1])\n",
    "dd_capacity = next((alpha for alpha, sr in zip(alpha_list, dd_results) if sr < 0.9), alpha_list[-1])\n",
    "\n",
    "print(f\"\\nCapacity comparison:\")\n",
    "print(f\"  Hebbian learning capacity: α ≈ {hebb_capacity:.3f}\")\n",
    "print(f\"  Daydreaming capacity: α ≈ {dd_capacity:.3f}\")\n",
    "if dd_capacity > hebb_capacity:\n",
    "    improvement = ((dd_capacity - hebb_capacity) / hebb_capacity) * 100\n",
    "    print(f\"  → Daydreaming improves capacity by ~{improvement:.0f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
